# NYT Connections Puzzle Solver

This repository contains code and data for analyzing language model performance on the New York Times Connections puzzles.

## Core Files

- `main.py` - The primary script that evaluates language models' ability to solve NYT Connections puzzles. It uses the `connections.json` dataset (which comes from a new GitHub dataset) for current puzzles.

- `analyze_json_results.py` - Visualization script that analyzes the results from running `main.py`. This generates performance metrics and plots stored in the `new_github_data_plots` directory.

- `connections.json` - Current puzzles dataset from a new GitHub source. This is the dataset used by `main.py` for evaluating models on new puzzles that models are not pretrained on.

- `new_puzzle_results.csv` - Results data generated by running `main.py` on the new GitHub puzzles dataset. Contains performance metrics for various models and prompt strategies.

- `cur_graphs/` - Contains visualization outputs generated by `analyze_json_results.py`. These charts show performance metrics on the newer puzzles that models are not pretrained on.


## Old files for original Kaggle data set (not in current workflow) 

- `connections_results.csv` - Results generated from the older kaggle data set

- `Connections_Data.csv` - The older Kaggle data set

- `plots/` - Contains visualization using the older Kaggle dataset.

## Workflow

1. Run `main.py` to evaluate models on NYT Connections puzzles using the `connections.json` dataset
2. Performance results are saved to `new_puzzle_results.csv`
3. Run `analyze_json_results.py` to generate visualizations in the `cur_graphs/` directory

Each model is evaluated using 3 different prompt strategies to measure performance variations across different approaches. 
